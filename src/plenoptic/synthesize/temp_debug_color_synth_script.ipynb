{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from typing import List, Tuple, Union, Callable, Optional\n",
    "import plenoptic as po\n",
    "import pyrtools as pt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../tools/')\n",
    "from plenoptic.tools import img_transforms \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= '../../../../../ceph/Datasets/select_color_textures_unsplash'\n",
    "save_path = '../../../../../ceph/experiments/color_texture_synth'\n",
    "# Function to load and preprocess images\n",
    "def load_and_reshape_images(image_path: str) -> torch.Tensor:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    images = []\n",
    "    for filename in glob.glob(os.path.join(image_path, '*.jpg')):  \n",
    "        img = Image.open(filename).convert('RGB')\n",
    "        img = transform(img)\n",
    "        img = img_transforms.rgb_to_opponentcone(img)\n",
    "        images.append(img)\n",
    "    print(\"Images loaded and preprocessed.\")\n",
    "    return torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "images = load_and_reshape_images(image_path)\n",
    "#images[0].shape[-1].to_numpy()\n",
    "# convert an int to a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_and_preprocess_images(images: torch.Tensor) -> torch.Tensor:\n",
    "    img_transforms.rgb_to_opponentcone(images)\n",
    "    # Rescale images to [0, 1]\n",
    "    images = images - images.min()\n",
    "    images = images / images.max()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_rescale_and_transform(images: torch.Tensor) -> List[Image.Image]:\n",
    "    rgb_images = img_transforms.opponentcone_to_rgb(images)\n",
    "    rgb_images = [transforms.ToPILImage()(img) for img in rgb_images]\n",
    "    print(\"Inverse rescale and transform complete.\")\n",
    "    return rgb_images\n",
    "\n",
    "\n",
    "# Main function to run synthesis and save images\n",
    "def main(model_name: str,max_iter: int = 300,init_image = None, \n",
    "        ctf_iters_to_check: int = 3, loss_function = po.tools.optim.l2_channelwise, coarse_to_fine: str = 'together', image_path: str = '../../../../../ceph/Datasets/select_color_textures_unsplash',save_path: str = '../../../../../ceph/experiments/color_texture_synth',):\n",
    "    \n",
    "    # TODO: Add these arguments from portillasimoncelli constructor:  \n",
    "    #n_scales: int = 4,\n",
    "    #n_orientations: int = 4,\n",
    "    #spatial_corr_width: int = 9\n",
    "    # crosschannel covariance\n",
    "    images = load_and_reshape_images(image_path)\n",
    "    images = rescale_and_preprocess_images(images)\n",
    "    if init_image is None:\n",
    "        init_image = torch.rand_like(images) * .01 + images.mean()\n",
    "\n",
    "\n",
    "    model = model_name(images[0,0,:,:].shape)\n",
    "\n",
    "    metamer = po.synth.MetamerCTF(\n",
    "        image=images,\n",
    "        model=model, \n",
    "        loss_function=loss_function, \n",
    "        initial_image=init_image, \n",
    "        coarse_to_fine=coarse_to_fine\n",
    "    )\n",
    "    \n",
    "    metamer.synthesize(max_iter=max_iter, ctf_iters_to_check=ctf_iters_to_check)\n",
    "    print(\"Synthesis complete.\")\n",
    "    synthesized_images = metamer.synthesized_image\n",
    "    \n",
    "    rgb_images = inverse_rescale_and_transform(synthesized_images)\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        print(f\"Created directory {save_path}\")\n",
    "    \n",
    "    for i, img in enumerate(rgb_images):\n",
    "        print(f\"Saving image {i}\")\n",
    "        save_filename = f\"{ctf_iters_to_check}_{model_name}_{max_iter}.png\"\n",
    "        img.save(os.path.join(save_path, save_filename))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import argparse\n",
    "#     # path to the data: /mnt/home/hdettki/ceph/Datasets/select_color_textures_unsplash\n",
    "#     # path to color script: /mnt/home/hdettki/code/plenoptic/src/plenoptic/synthesize\n",
    "#     parser = argparse.ArgumentParser(description=\"Run image synthesis with specified parameters.\")\n",
    "#     parser.add_argument(\"--image_path\", type=str, required=False, default= '../../../../../ceph/Datasets/select_color_textures_unsplash',help=\"Path to the input images.\")\n",
    "#     parser.add_argument(\"--save_path\", type=str, required=False,default='../../../../../ceph/experiments/color_texture_synth', help=\"Path to save the output images.\")\n",
    "#     parser.add_argument(\"--model_name\", type=str, required=True, help=\"Model to be used.\")\n",
    "#     #parser.add_argument(\"--loss_function\", type=str, required=False, help=\"Loss function to be used. Recommended: l2channelwise in po.tools.optim\")\n",
    "#     #parser.add_argument(\"--init_image\", type=None, required=False, help=\"Initial image for synthesis.\")\n",
    "#     parser.add_argument(\"--coarse_to_fine\", type=bool, default=False, help=\"Use coarse to fine synthesis.\")\n",
    "#     parser.add_argument(\"--max_iter\", type=int, required=False,default=300, help=\"Maximum number of iterations. If GPU available, use > 3000.\")\n",
    "#     parser.add_argument(\"--ctf_iters_to_check\", type=int, nargs='+', required=False,default=3, help=\"Iterations to check in coarse to fine synthesis.\")\n",
    "    \n",
    "\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "    # main(\n",
    "    #     image_path=args.image_path, \n",
    "    #     save_path=args.save_path, \n",
    "    #     model_name=getattr(po.simul, args.model_name), \n",
    "    #     #loss_function=getattr(po.tools.optim, args.loss_function), \n",
    "    #     #init_image=torch.load(args.init_image) if args.init_image else None, \n",
    "    #     coarse_to_fine=args.coarse_to_fine, \n",
    "    #     max_iter=args.max_iter, \n",
    "    #     ctf_iters_to_check=args.ctf_iters_to_check\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded and preprocessed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/hdettki/code/plenoptic/src/plenoptic/tools/validate.py:178: UserWarning: model is in training mode, you probably want to call eval() to switch to evaluation mode\n",
      "  warnings.warn(\n",
      "/mnt/home/hdettki/code/plenoptic/src/plenoptic/tools/validate.py:211: UserWarning: Validating whether model can work with coarse-to-fine synthesis -- this can take a while!\n",
      "  warnings.warn(\"Validating whether model can work with coarse-to-fine synthesis -- this can take a while!\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b409021ea329468ea9efce32ce5d7165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 5 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPortillaSimoncelliCrossChannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctf_iters_to_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_name, max_iter, init_image, ctf_iters_to_check, loss_function, coarse_to_fine, image_path, save_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m model_name(images[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,:,:]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     25\u001b[0m metamer \u001b[38;5;241m=\u001b[39m po\u001b[38;5;241m.\u001b[39msynth\u001b[38;5;241m.\u001b[39mMetamerCTF(\n\u001b[1;32m     26\u001b[0m     image\u001b[38;5;241m=\u001b[39mimages,\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     coarse_to_fine\u001b[38;5;241m=\u001b[39mcoarse_to_fine\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmetamer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctf_iters_to_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctf_iters_to_check\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSynthesis complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m synthesized_images \u001b[38;5;241m=\u001b[39m metamer\u001b[38;5;241m.\u001b[39msynthesized_image\n",
      "File \u001b[0;32m~/code/plenoptic/src/plenoptic/synthesize/metamer.py:654\u001b[0m, in \u001b[0;36mMetamerCTF.synthesize\u001b[0;34m(self, max_iter, optimizer, scheduler, store_progress, stop_criterion, stop_iters_to_check, change_scale_criterion, ctf_iters_to_check)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# update saved_* attrs. len(losses) gives the total number of\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;66;03m# iterations and will be correct across calls to `synthesize`\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses))\n\u001b[0;32m--> 654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchange_scale_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctf_iters_to_check\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(loss):\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound a NaN in loss during optimization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/plenoptic/src/plenoptic/synthesize/metamer.py:714\u001b[0m, in \u001b[0;36mMetamerCTF._optimizer_step\u001b[0;34m(self, pbar, change_scale_criterion, ctf_iters_to_check)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;66;03m# reset ctf target representation, so we update it on\u001b[39;00m\n\u001b[1;32m    712\u001b[0m             \u001b[38;5;66;03m# next pass\u001b[39;00m\n\u001b[1;32m    713\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctf_target_representation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m loss, overall_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_closure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scales_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_losses\u001b[38;5;241m.\u001b[39mappend(overall_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.virtualenvs/plenoptic/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/plenoptic/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.virtualenvs/plenoptic/lib/python3.10/site-packages/torch/optim/adam.py:148\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 148\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    151\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/code/plenoptic/src/plenoptic/synthesize/metamer.py:784\u001b[0m, in \u001b[0;36mMetamerCTF._closure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# this is just for display, so don't compute gradients\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 784\u001b[0m         overall_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     target_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/plenoptic/src/plenoptic/synthesize/metamer.py:225\u001b[0m, in \u001b[0;36mMetamer.objective_function\u001b[0;34m(self, metamer_representation, target_representation)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_representation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     target_representation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_representation\n\u001b[0;32m--> 225\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetamer_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtarget_representation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m range_penalty \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mpenalize_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetamer,\n\u001b[1;32m    228\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_range)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrange_penalty_lambda \u001b[38;5;241m*\u001b[39m range_penalty\n",
      "File \u001b[0;32m~/code/plenoptic/src/plenoptic/tools/optim.py:99\u001b[0m, in \u001b[0;36ml2_channelwise\u001b[0;34m(synth_rep, ref_rep, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m channel_losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(ref_rep \u001b[38;5;241m-\u001b[39m synth_rep, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# print(f'channel losses {channel_losses}, channel loss shape: {channel_losses.shape}')\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 5 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "main(po.simul.PortillaSimoncelliCrossChannel, max_iter=300, ctf_iters_to_check=3, image_path=image_path, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plenoptic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
