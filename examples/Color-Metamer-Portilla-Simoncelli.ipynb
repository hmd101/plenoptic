{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Resize\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import plenoptic as po\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import os.path as op\n",
    "import einops\n",
    "from typing import Union, Tuple, Callable, List, Dict, Optional\n",
    "import glob\n",
    "import math\n",
    "import pyrtools as pt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "# We need to download some additional images for this notebook. In order to do so,\n",
    "# we use an optional dependency, pooch. If the following raises an ImportError or ModuleNotFoundError \n",
    "# then install pooch in your plenoptic environment and restart your kernel.\n",
    "DATA_PATH = po.data.fetch_data('portilla_simoncelli_images.tar.gz')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# so that relative sizes of axes created by po.imshow and others look right\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "\n",
    "# set seed for reproducibility\n",
    "po.tools.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# These variables control how long metamer synthesis runs for. The values present here will result in completed synthesis,\n",
    "# but you may want to decrease these numbers if you're on a machine with limited resources.\n",
    "short_synth_max_iter = 100\n",
    "long_synth_max_iter = 300\n",
    "longest_synth_max_iter = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portilla-Simoncelli Texture Metamer\n",
    "\n",
    "In this tutorial we will aim to replicate [Portilla & Simoncelli (1999)](https://www.cns.nyu.edu/pub/eero/portilla99-reprint.pdf). The tutorial is broken into the following parts:\n",
    "\n",
    "1. Introduce the concept of a Visual Texture.\n",
    "2. How to synthesize metamers for the Portilla & Simoncelli texture model. \n",
    "3. Demonstrate the importance of different classes of statistics.\n",
    "4. Extending the model to synthesizing color metamers\n",
    "\n",
    "\n",
    "Note that this notebook takes a long time to run (roughly an hour with a GPU, several hours without), because of all the metamers that are synthesized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. What is a visual texture?\n",
    "\n",
    "The simplest definition is a repeating visual pattern. Textures encompass a wide variety of images, including natural patterns such as bark or fur, artificial ones such as brick, and computer-generated ones such as the Julesz patterns ([Julesz 1978](https://link.springer.com/article/10.1007/BF00336998), [Yellot 1993](https://opg.optica.org/josaa/abstract.cfm?uri=josaa-10-5-777)). Below we load some examples.  \n",
    "\n",
    "The Portilla-Simoncelli model was developed to measure the statistical properties of visual textures.  Metamer synthesis was used (and can be used) in conjunction with the Portilla-Simoncelli texture model to demonstrate the necessity of different properties of the visual texture.  We will use some of these example textures to demonstrate aspects of the Portilla Simoncelli model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the remainder of the notebook we will use this helper function to\n",
    "# run synthesis so that the cells are a bit less busy.\n",
    "\n",
    "# Be sure to run this cell.\n",
    "\n",
    "\n",
    "def run_synthesis(img, model, loss_function:Callable[[torch.Tensor, torch.Tensor], torch.Tensor], im_init=None):\n",
    "    r\"\"\" Performs synthesis with the full Portilla-Simoncelli model. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            A tensor containing an img.\n",
    "        model :\n",
    "            A model to constrain synthesis.\n",
    "        im_init: Tensor\n",
    "            A tensor to start image synthesis.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        met: Metamer\n",
    "            Metamer from the full Portilla-Simoncelli Model\n",
    "\n",
    "        \"\"\"\n",
    "    if im_init is None:\n",
    "        im_init = torch.rand_like(img) * .01 + img.mean()\n",
    "    met = po.synth.MetamerCTF(img, model, loss_function=loss_function, initial_image=im_init,\n",
    "                              coarse_to_fine='together')\n",
    "    met.synthesize(\n",
    "        max_iter=long_synth_max_iter, \n",
    "        store_progress=True,\n",
    "        change_scale_criterion=None,\n",
    "        ctf_iters_to_check=3, # todo: tinker with this\n",
    "        )\n",
    "    return met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Portilla-Simoncelli Model to Compute Realistic Color Metamers\n",
    "\n",
    "Before adapting the model to synthesize metamers better on color texture images, let's have a look at the existing model's behavior.\n",
    "\n",
    "### Combining Channel Information in RGB-Images\n",
    "- The current implementation of the Portilla-Simoncelli Model computes the statistics on all channels separately, i.e., treats each channel as a separate image, resulting in unrealistic metamers (see demo below).\n",
    "- This is why we will play around with color transforms in the forward method.\n",
    "- We will first mmic what [Brown 2023](https://dl.acm.org/doi/full/10.1145/3564605#sec-supp) et al. did.\n",
    "- In particular, we will write our custom Portilla-Simoncelli class, where we do the following steps:\n",
    "\n",
    "#### Implementation:\n",
    "0. **Transform to LMS**:\n",
    "    - transform RGB to an LMS-like space, then from that into opposing color channels (define rgb2opc)\n",
    "1. **Channel Separation**: The input image is split into its constituent channels.\n",
    "    - compute L2 loss separately on each channel, the use torch.logsumexp to comine them\n",
    "2. **Independent Processing**: Each channel is processed independently to compute the relevant statistics.\n",
    "    - then add cross-channel correlations as in Brown\n",
    "3. **Normalization and Weighting**: The computed statistics are normalized and weighted to ensure they contribute appropriately to the final representation. E.g., the green channel might be assigned higher weights as it  is often more sensitive in human vision.\n",
    "4. **Fusion and Integration**: The normalized and weighted statistics are combined to form the final multi-channel representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesis on Color-Texture Images\n",
    "#### 1. Synthesis with Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_unsplash = '../../../ceph/Datasets/select_color_textures_unsplash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import select color images\n",
    "\n",
    "# Step 1: Check if the path to the images exists\n",
    "# You can specify the extensions of the images you want to load, e.g., *.jpg, *.png, etc.\n",
    "image_files = glob.glob(os.path.join(path_to_unsplash, '*.jpg'))  # Assuming images are in .jpg format\n",
    "image_files.extend(glob.glob(os.path.join(path_to_unsplash, '*.png')))  # Include .png format as well\n",
    "\n",
    "# Step 2: Load the images\n",
    "images = []\n",
    "for image_file in image_files:\n",
    "    try:\n",
    "        img = Image.open(image_file)\n",
    "        images.append(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_file}: {e}\")\n",
    "#po.imshow(po.tools.load_images(path_to_unsplash, as_gray=False), as_rgb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the images to tensors and then resize them to 256x256\n",
    "#images = [torch.tensor(np.array(img)).permute(2, 0, 1).unsqueeze(0).float() / 255.0 for img in images]\n",
    "# Define a sequence of transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # resize to 256x256\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Apply the transformations\n",
    "img_tensor_ls = []\n",
    "for img in images:\n",
    "    img_tensor_ls.append(transform(img))\n",
    "\n",
    "img_tensor = torch.stack(img_tensor_ls, dim=0) # stack the images along the batch dimension\n",
    "# send images to GPU, if available\n",
    "img_tensor = img_tensor.to(DEVICE)\n",
    "\n",
    "\n",
    "# for each image, run synthesis with full PortillaSimoncelli model\n",
    "model = po.simul.PortillaSimoncelli(img_tensor.shape[-2:]).to(DEVICE)\n",
    "metamers = []\n",
    "for i in range(img_tensor.shape[0]-4): # only run synthesis for the first, 5 images in the batch\n",
    "    # metamer = run_synthesis(img_tensor[i:i+1], model)\n",
    "    metamer = run_synthesis(img_tensor[i:i+1], model,loss_function=po.tools.optim.l2_norm)\n",
    "    metamers.append(metamer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all images in img_tensor\n",
    "po.imshow(img_tensor, as_rgb=True)\n",
    "img_tensor.min(), img_tensor.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the metamer images \n",
    "po.imshow([metamer.image for metamer in metamers], title='Target images', as_rgb=True)\n",
    "# print target image\n",
    "po.imshow([metamer.metamer for metamer in metamers], title='Synthesized metamer images', as_rgb=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the above metamers show some discrepancies to the target image, some more, some less. While images 2, 3, and 4 approximate the texture of the target image quite well, the colors are a bit off, especially in image 4. Metamers 1 and 5 are off in both color and texture. It seems that homegeneous, finer patterns are well represented by the model, while distinct coarser structures, such as the arteries in the leaves or the cracks in the wood, are captured, but not in the same location. The result is a combined texture that doesn't reflect the original in the physical world. \n",
    "\n",
    "So let's, first tweak the model so that it performs better in terms of color in the metamers.\n",
    "\n",
    "#### 2. Adapting the Model to Produce Better Color Metamers\n",
    "**Combining Channel Information in RGB-Images**\n",
    "- The current implementation of the Portilla-Simoncelli Model computes the statistics on all channels separately, i.e., treats each channel as a separate image, resulting in unrealistic metamers (see demo below).\n",
    "- This is why we will play around with color transforms in the forward method.\n",
    "- We will first mmic what [Brown 2023](https://dl.acm.org/doi/full/10.1145/3564605#sec-supp) et al. did.\n",
    "- In particular, we will write our custom Portilla-Simoncelli class, where we do the following steps:\n",
    "\n",
    "##### Implementation:\n",
    "0. **Transform to LMS**:\n",
    "    - transform RGB to an LMS-like space, then from that into opposing color channels (define rgb2opc)\n",
    "    - Most of the color transforms are copied from color_utils.py in [PooledStatisticsMetamers](https://github.com/ProgramofComputerGraphics/PooledStatisticsMetamers/blob/main/poolstatmetamer/color_utils.py#L58) repo \n",
    "1. **Channel Separation**: The input image is split into its constituent channels.\n",
    "    - compute L2 loss separately on each channel, the use torch.logsumexp to comine them\n",
    "2. **Independent Processing**: Each channel is processed independently to compute the relevant statistics.\n",
    "    - then add cross-channel correlations as in Brown\n",
    "3. **Normalization and Weighting**: The computed statistics are normalized and weighted to ensure they contribute appropriately to the final representation. E.g., the green channel might be assigned higher weights as it  is often more sensitive in human vision.\n",
    "4. **Fusion and Integration**: The normalized and weighted statistics are combined to form the final multi-channel representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first take care of the transforms\n",
    "# A approximate transform from a typical RGB space to cone LMS space\n",
    "# Would be good to check this to see if is a reasonable default approximation for \n",
    "# human cone LMS, but results so far don't seem to depend much on precise color transform details\n",
    "# code largely copied from color_utils in PooledStatisticsMetamers repo \n",
    "rgb2lms = torch.tensor([[0.3811, 0.5783, 0.0402],\n",
    "                        [0.1967, 0.7244, 0.0782],\n",
    "                        [0.0241, 0.1288, 0.8444]])\n",
    "lms2rgb = torch.inverse(rgb2lms)\n",
    "# A simple approximation of the opponent cone color space (achromatic,red-green,blue-yellow)\n",
    "# I also scaled their magnitudes to make the channels have more simlar ranges\n",
    "lms2opc = torch.tensor([[0.5, 0.5, 0],    # (L+M) / 2)\n",
    "                       [-4, 4, 0],        # (M-L) * 3\n",
    "                       [0.5, 0.5, -1]])  # (L+M)/2 - S)\n",
    "\n",
    "opc2lms = lms2opc.inverse()\n",
    "\n",
    "# Composite transform from RGB to cone-opponent color space\n",
    "rgb2opc = torch.matmul(lms2opc,rgb2lms)\n",
    "\n",
    "opc2rgb = rgb2opc.inverse()\n",
    "\n",
    "# Short names for the three opponent channels, short for (achromatic,red-green,blue-yellow)\n",
    "opc_short_names = ('ac','rg','by')\n",
    "\n",
    "\n",
    "# Apply the specified color tranformation matrix to an image\n",
    "def color_transform_image(image,color_matrix):\n",
    "    if image.dim()==3:\n",
    "        return torch.nn.functional.conv2d(image[None,:,:,:],color_matrix[:,:,None,None])\n",
    "    else:\n",
    "        return torch.nn.functional.conv2d(image,color_matrix[:,:,None,None])\n",
    "\n",
    "    \n",
    "# Convert an RGB image to cone LMS image\n",
    "def rgb_to_coneLMS(image):\n",
    "    return color_transform_image(image,rgb2lms)\n",
    "\n",
    "# Convert an RGB image to a cone opponent space image\n",
    "def rgb_to_opponentcone(image):\n",
    "    return color_transform_image(image,rgb2opc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Single wood image\n",
    "wood_img = img_tensor[0].unsqueeze(dim=0).to('cpu')\n",
    "opc_image = einops.einsum(wood_img, rgb2opc, 'b c1 h w, c2 c1 -> b c2 h w') # tensor product of image and color matrix\n",
    "# transform opc image to rgb\n",
    "opc_image_retransf = einops.einsum(opc_image, opc2rgb, 'b c1 h w, c2 c1 -> b c2 h w')\n",
    "po.imshow(opc_image, title='Cone opponent image', as_rgb=True)\n",
    "po.imshow(opc_image_retransf, title='Cone opponent image retransformed to RGB', as_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and move tensors to GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move img_tensor and rgb2opc to the chosen device\n",
    "img_tensor = img_tensor.to(device)\n",
    "rgb2opc = rgb2opc.to(device)\n",
    "\n",
    "# Convert the images to cone opponent space\n",
    "img_opc_tensor = color_transform_image(img_tensor, rgb2opc)\n",
    "po.imshow(img_opc_tensor, title='Cone opponent images', as_rgb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(image: torch.Tensor, range: tuple = (0, 1)):\n",
    "    # Compute the min and max values across height and width for each image in the batch\n",
    "    min_val = image.amin(dim=(2, 3), keepdim=True)\n",
    "    max_val = image.amax(dim=(2, 3), keepdim=True)\n",
    "    \n",
    "    # Rescale the images to the range [0, 1]\n",
    "    image = (image - min_val) / (max_val - min_val + 1e-8)  # Adding epsilon to avoid division by zero\n",
    "    \n",
    "    return image, min_val, max_val\n",
    "def inverse_rescale(image: torch.Tensor, min_val: torch.Tensor, max_val: torch.Tensor):\n",
    "    # Reverse the rescaling to original values\n",
    "    image = image * (max_val - min_val) + min_val\n",
    "    return image\n",
    "\n",
    "# Compute rescaled images and min/max values\n",
    "img_opc_tensor_rescaled, min_val, max_val = rescale(img_opc_tensor)\n",
    "img_opc_tensor_resc_inv = inverse_rescale(img_opc_tensor_rescaled, min_val, max_val)\n",
    "# Sanity check: the original and rescaled images should be the same\n",
    "# Check if the inverse rescaling works correctly\n",
    "torch.allclose(img_opc_tensor, img_opc_tensor_resc_inv, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the rescaled opc images\n",
    "po.imshow(img_opc_tensor_rescaled, title='Rescaled cone opponent images', as_rgb=True)\n",
    "# show the original opc images\n",
    "po.imshow(img_opc_tensor, title='Original cone opponent images', as_rgb=True)\n",
    "# Sanity check: the original and rescaled images should be the same\n",
    "# show the inverse rescaled opc images\n",
    "#po.imshow(img_opc_tensor_resc_inv, title='Inverse rescaled cone opponent images', as_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# single wood image:\n",
    "# rescale the image to the range [0, 1] on final two dimensions\n",
    "# opc_img_rescaled, min_val, max_val = rescale(opc_image) # single wood image\n",
    "# po.imshow(opc_img_rescaled, title='Cone opponent image rescaled', as_rgb=True) # this is where we conduct the metamer synthesis on\n",
    "# opc_img_inverse_rescaled = inverse_rescale(opc_img_rescaled, min_val, max_val)\n",
    "# po.imshow(opc_img_inverse_rescaled, title='Cone opponent image inverse rescaled', as_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Min and max values of the ORIGINAL images: {img_tensor.amin(dim=(2, 3)).squeeze()} and {img_tensor.amax(dim=(2, 3)).squeeze()}\")\n",
    "print(f\"Min and max values of the OPC images: {img_opc_tensor.amin(dim=(2, 3)).squeeze()} and {img_opc_tensor.amax(dim=(2, 3)).squeeze()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram for each image in the batch of rescaled images\n",
    "# def plot_histograms(images: torch.Tensor,  title: str,bins: int = 100):\n",
    "#     # Create a figure and axis\n",
    "#     fig, ax = plt.subplots(1, images.shape[0], figsize=(15, 5))\n",
    "#     fig.suptitle(title)\n",
    "    \n",
    "#     # Plot the histograms for each image in the batch\n",
    "#     for i in range(images.shape[0]):\n",
    "#         ax[i].hist(images[i].flatten().cpu().numpy(), bins=bins)\n",
    "#         ax[i].set_title(f'Image {i + 1}')\n",
    "#         ax[i].set_xlabel('Pixel Intensity')\n",
    "#         ax[i].set_ylabel('Frequency')\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# plot histogram for each image in the batch of rescaled images\n",
    "def plot_histograms(images: torch.Tensor, title: str, bins: int = 100):\n",
    "    # Ensure images are on CPU and converted to numpy arrays\n",
    "    images_np = images.cpu().numpy()\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, axs = plt.subplots(images.shape[0], images.shape[1], figsize=(21, 9), sharex=True)\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Plot the histograms for each channel of each image in the batch\n",
    "    for i in range(images.shape[0]):  # iterate over images\n",
    "        for j in range(images.shape[1]):  # iterate over channels\n",
    "            axs[i, j].hist(images_np[i, j].flatten(), bins=bins)\n",
    "            axs[i, j].set_title(f'Image {i + 1}, Channel {j + 1}')\n",
    "            axs[i, j].set_xlabel('Pixel Intensity')\n",
    "            axs[i, j].set_ylabel('Frequency')\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# histograms of the rgb images \n",
    "plot_histograms(img_tensor, title='Histograms of RGB Images')\n",
    "#  histograms of the opc images prio to rescaling\n",
    "plot_histograms(img_opc_tensor, title='Histograms of Cone Opponent Images')\n",
    "#  histograms of the rescaled images\n",
    "plot_histograms(img_opc_tensor_rescaled, title='Histograms of Rescaled Cone Opponent Images')\n",
    "# print the min and max value for each image in the batch of rescaled images, opc images and rgb images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom Loss function\n",
    "- compute the L2 norm on each channel separately \n",
    "- then combine channel losses with logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_channelwise(synth_rep: torch.Tensor, ref_rep: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "    r\"\"\"l2-norm of the difference between ref_rep and synth_rep per channel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    synth_rep\n",
    "        The first tensor to compare, model representation of the\n",
    "        synthesized image.\n",
    "    ref_rep\n",
    "        The second tensor to compare, model representation of the\n",
    "        reference image. must be same size as ``synth_rep``.\n",
    "    kwargs\n",
    "        Ignored, only present to absorb extra arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss\n",
    "        The L2-norm of the difference between ``ref_rep`` and ``synth_rep``.\n",
    "\n",
    "    \"\"\"\n",
    "    channel_losses = torch.linalg.vector_norm(ref_rep - synth_rep, dim=2,ord=2)\n",
    "    # print(f'channel losses {channel_losses}, channel loss shape: {channel_losses.shape}')\n",
    "    return torch.logsumexp(channel_losses, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The following class tweaks the PortillaSimoncelli model so that it will process color images better.\n",
    "#  In particular, we introduce cross-color channel statistics, to capture the relationship between different color channels.\n",
    "#  The cross-color statistics are a reduced set of statistis, currently only encompassing covariance/correlation. \n",
    "from typing import List, Optional, Any\n",
    "from collections import OrderedDict\n",
    "\n",
    "SCALES_TYPE = Optional[List[int]]\n",
    "\n",
    "#from collections import OrderedDict\n",
    "class PortillaSimoncelliCrossChannel(po.simul.PortillaSimoncelli):\n",
    "    r\"\"\"Model for measuring a subset of texture statistics reported by PortillaSimoncelli\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im_shape: int\n",
    "        the size of the images being processed by the model, should be divisible by 2^n_scales\n",
    "    remove_keys: list\n",
    "        The dictionary keys for the statistics we will \"remove\".  In practice we set them to zero.\n",
    "        Possible keys: [\"pixel_statistics\", \"auto_correlation_magnitude\",\n",
    "        \"skew_reconstructed\", \"kurtosis_reconstructed\", \"auto_correlation_reconstructed\", \n",
    "        \"std_reconstructed\", \"magnitude_std\", \"cross_orientation_correlation_magnitude\", \n",
    "        \"cross_scale_correlation_magnitude\" \"cross_scale_correlation_real\", \"var_highpass_residual\"]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        im_shape,\n",
    "    ):\n",
    "        super().__init__(im_shape, n_scales=4, n_orientations=4, spatial_corr_width=9)\n",
    "        \n",
    "    def forward(self, image: torch.Tensor, scales: Optional[List[SCALES_TYPE]] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate Texture Statistics representation of an image with cross-channel statistics.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : Tensor\n",
    "            A 4d tensor (batch, channel, height, width) containing the image(s) to analyze.\n",
    "        scales : List[SCALES_TYPE], optional\n",
    "            Which scales to include in the returned representation. If None, include all scales.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        representation_tensor : Tensor\n",
    "            3d tensor of shape (batch, channel, stats) containing the measured texture statistics.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `image` is not 4d or has a dtype other than float or complex.\n",
    "        \"\"\"\n",
    "\n",
    "        # call the parent class forward method to compute the base statistics\n",
    "        base_representations = super().forward(image, scales=scales)\n",
    "\n",
    "        # compute the cross-channel statistics\n",
    "        cross_channel_stats = self._compute_cross_channel_stats(image)\n",
    "\n",
    "        # modidfy the base representation to include the cross-channel statistics\n",
    "        representation_tensor = torch.cat((base_representations, cross_channel_stats), dim=-1)\n",
    "\n",
    "        # unnormalize representation_tensor\n",
    "        # representation_tensor = representation_tensor * (max_val - min_val) + min_val\n",
    "\n",
    "        return representation_tensor\n",
    "\n",
    "    def _compute_cross_channel_stats(self, image:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute cross-channel statistics for the input image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : torch.Tensor\n",
    "            A 4d tensor (batch, channel, height, width) containing the image(s) to analyze.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            3d tensor of shape (batch, channel, stats) containing the cross-channel statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        ## compute the cross-channel statistics\n",
    "        cross_channel_stats = self._compute_cross_channel_covariance(image)\n",
    "\n",
    "\n",
    "\n",
    "        return cross_channel_stats\n",
    "    \n",
    "    def _compute_cross_channel_covariance(self, image:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the cross-channel covariance for the input image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : torch.Tensor\n",
    "            A 4d tensor (batch, channel, height, width) containing the image(s), potentially in some preprocessed state like cone LMS, OPC space.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            3d tensor of shape (batch, channel, stats) containing the cross-channel covariance statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the mean across the channel dimension\n",
    "        mean_across_channels = image.mean(dim=(2, 3), keepdim=True)  # shape: [batch_size, num_channels, 1, 1]\n",
    "\n",
    "        # Centering the data\n",
    "        centered_data = image - mean_across_channels\n",
    "\n",
    "        # Vectorized computation of covariance matrix\n",
    "        covariance_matrix = (centered_data[:, :, None, :, :] * centered_data[:, None, :, :, :]).mean(dim=(3, 4))\n",
    "\n",
    "        #print(f'covariance matrix requires grad: {covariance_matrix.requires_grad}')\n",
    "\n",
    "        return 1e5*covariance_matrix\n",
    "        \n",
    "    # overwriting the following two methods allows us to use the plot_representation method\n",
    "    # with the modified model, making examining it easier. In particular, it will add the new cross-channel statistics to the plot.\n",
    "    def convert_to_dict(self, representation_tensor: torch.Tensor) -> OrderedDict:\n",
    "        \"\"\"Convert tensor of stats to dictionary.\"\"\"\n",
    "        #n_cross_channel_cov = self.n_scales * self.n_orientations\n",
    "        n_cross_channel_cov = self.num_channels ** 2\n",
    "        rep = super().convert_to_dict(representation_tensor[..., :-n_cross_channel_cov])\n",
    "        cross_channel_cov = representation_tensor[..., -n_cross_channel_cov:]\n",
    "        rep['cross_channel_covariance'] = einops.rearrange(cross_channel_cov, 'b c (s o) -> b c s o', s=self.n_scales, o=self.n_orientations)\n",
    "        return rep\n",
    "\n",
    "    def _representation_for_plotting(self, rep: OrderedDict) -> OrderedDict:\n",
    "        r\"\"\"Convert the data into a dictionary representation that is more convenient for plotting.\n",
    "\n",
    "        Intended as a helper function for plot_representation.\n",
    "        \"\"\"\n",
    "        cross_channel_cov = rep.pop('cross_channel_covariance')\n",
    "        data = super()._representation_for_plotting(rep)\n",
    "        data['cross_channel_covariance'] = cross_channel_cov.flatten()\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming opc_images_rescaled is a batch of image tensors with shape [batch_size, channels, height, width]\n",
    "\n",
    "# Create the model instance outside the loop\n",
    "model = PortillaSimoncelliCrossChannel(img_opc_tensor_rescaled.shape[-2:]).to(DEVICE)\n",
    "\n",
    "# Move all images to the desired device (assuming DEVICE is defined somewhere)\n",
    "img_batch = img_opc_tensor_rescaled.to(DEVICE)\n",
    "\n",
    "# number of target images in the batch\n",
    "n_avail_imgs = img_batch.shape[0]\n",
    "nth_img = n_avail_imgs # number of images in the batch\n",
    "# or choose a number smaller than n_avail_imgs to accelaterate synthesis\n",
    "#nth_img = 1\n",
    "\n",
    "\n",
    "# Run synthesis on all images in batch\n",
    "# metamers_opc = run_synthesis(img_batch, model, loss_function=l2_channelwise)\n",
    "# initialize the an empty list to store the metamers\n",
    "metamers_opc = []\n",
    "for img in img_batch[0:nth_img]:\n",
    "    print(img.shape)\n",
    "    metamer = run_synthesis(img[None, :, :,:], model, loss_function=l2_channelwise)\n",
    "    metamers_opc.append(metamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the metamers to a tensor\n",
    "metamers_opc_tensor = torch.stack([metamer.metamer for metamer in metamers_opc], dim=0)\n",
    "# drop the first dimension of the tensor\n",
    "metamers_opc_tensor = metamers_opc_tensor.squeeze(1)\n",
    "metamers_opc_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print opc metamer images and target images\n",
    "po.imshow([metamer.image for metamer in metamers_opc], title='Target images, OPC, scaled', as_rgb=True)\n",
    "po.imshow(metamers_opc_tensor, title='metamer images in OPC, scaled', as_rgb=True)\n",
    "\n",
    "# inverse rescale metamer opc images\n",
    "metamers_opc_tensor_inverse_rescaled = inverse_rescale(metamers_opc_tensor, min_val[:nth_img], max_val[:nth_img])\n",
    "# compute the rgb metamer images\n",
    "opc2rgb = opc2rgb.to(DEVICE)\n",
    "metamers_rgb_tensor = color_transform_image(metamers_opc_tensor_inverse_rescaled, opc2rgb)\n",
    "# print the rgb metamer images\n",
    "po.imshow(metamers_rgb_tensor, title='metamer images in RGB', as_rgb=True)\n",
    "# print the original image\n",
    "po.imshow(img_tensor, title='Original Target images', as_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot errors\n",
    "[po.synth.metamer.plot_synthesis_status(metamer) for metamer in metamers_opc]\n",
    "#po.synth.metamer.plot_synthesis_status(metamers_rgb_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "plenoptic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
